{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bbe0f895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/JyeProject/lib/python3.10/site-packages (4.64.1)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/JyeProject/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/JyeProject/lib/python3.10/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/anaconda3/envs/JyeProject/lib/python3.10/site-packages (from pandas) (1.24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/anaconda3/envs/JyeProject/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/JyeProject/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cd0b85e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kyle-\n",
      "[nltk_data]     fogarty/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/kyle-fogarty/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declare import statements\n",
    "import nltk\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  \n",
    "from tqdm import tqdm\n",
    "# Download nltk addons\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c13edb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_loc = '/Users/kyle-fogarty/Documents/EFCAMDAT/'\n",
    "xml_file_name = 'Arab_All_Levels.xml'\n",
    "xml_file = minidom.parse('{}{}'.format(file_loc, xml_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b712de4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the xml file into pandas dataframe\n",
    "\n",
    "grades = xml_file.getElementsByTagName('grade')\n",
    "writing = xml_file.getElementsByTagName('writing')\n",
    "text_xml = xml_file.getElementsByTagName('text')\n",
    "learner = xml_file.getElementsByTagName('learner')\n",
    "topic = xml_file.getElementsByTagName('topic')\n",
    "\n",
    "grade = [] \n",
    "text = []\n",
    "ids = []\n",
    "writing_ids = []\n",
    "levels = []\n",
    "unit = []\n",
    "nationality = []\n",
    "topic_id = []\n",
    "topic_text = []\n",
    "\n",
    "for ind in range(len(grades)):\n",
    "    grade.append(float(grades[ind].firstChild.nodeValue))\n",
    "    text.append((text_xml[ind].firstChild.nodeValue))\n",
    "    \n",
    "    ids.append(int(learner[ind].attributes['id'].value))\n",
    "    nationality.append(str(learner[ind].attributes['nationality'].value))\n",
    "    \n",
    "    writing_ids.append(int(writing[ind].attributes['id'].value))\n",
    "    levels.append(int(writing[ind].attributes['level'].value))\n",
    "    unit.append(int(writing[ind].attributes['unit'].value))\n",
    "    \n",
    "    topic_id.append(int(topic[ind].attributes['id'].value))\n",
    "    topic_text.append(str(topic[ind].firstChild.nodeValue))\n",
    "\n",
    "ensemble = {\n",
    "  \"learner_id\": ids,\n",
    "  \"grades\": grade,\n",
    "  \"nationality\": nationality,\n",
    "  \"writing_id\": writing_ids,\n",
    "  \"levels\": levels,\n",
    "  \"unit\":unit,\n",
    "  \"topic_id\": topic_id,\n",
    "  \"topic_text\":topic_text,\n",
    "  \"passage\": text,}\n",
    "\n",
    "corpus = pd.DataFrame(ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d91ba275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learner_id</th>\n",
       "      <th>grades</th>\n",
       "      <th>nationality</th>\n",
       "      <th>writing_id</th>\n",
       "      <th>levels</th>\n",
       "      <th>unit</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>topic_text</th>\n",
       "      <th>passage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1801</td>\n",
       "      <td>80.0</td>\n",
       "      <td>kw</td>\n",
       "      <td>153</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Taking inventory in the office</td>\n",
       "      <td>\\n      Dear Ms There are 30 pens , 15 pens , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1801</td>\n",
       "      <td>89.0</td>\n",
       "      <td>kw</td>\n",
       "      <td>154</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Writing an online profile</td>\n",
       "      <td>\\n      Hi ! My name's Anas I'm 25 years old ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43077</td>\n",
       "      <td>80.0</td>\n",
       "      <td>sa</td>\n",
       "      <td>210</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>Writing about what you do</td>\n",
       "      <td>\\n      My name is Saad . l am 21 years old . ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>166681</td>\n",
       "      <td>90.0</td>\n",
       "      <td>sa</td>\n",
       "      <td>281</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>Writing about what you do</td>\n",
       "      <td>\\n      I'm working housewife, cleaner home, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166681</td>\n",
       "      <td>73.0</td>\n",
       "      <td>sa</td>\n",
       "      <td>282</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>Describing routines</td>\n",
       "      <td>\\n      Hello,  I'm  Malak , I'm working house...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   learner_id  grades nationality  writing_id  levels  unit  topic_id  \\\n",
       "0        1801    80.0          kw         153       1     2         2   \n",
       "1        1801    89.0          kw         154       1     3         3   \n",
       "2       43077    80.0          sa         210       4     1        25   \n",
       "3      166681    90.0          sa         281       4     1        25   \n",
       "4      166681    73.0          sa         282       4     2        26   \n",
       "\n",
       "                       topic_text  \\\n",
       "0  Taking inventory in the office   \n",
       "1       Writing an online profile   \n",
       "2       Writing about what you do   \n",
       "3       Writing about what you do   \n",
       "4             Describing routines   \n",
       "\n",
       "                                             passage  \n",
       "0  \\n      Dear Ms There are 30 pens , 15 pens , ...  \n",
       "1  \\n      Hi ! My name's Anas I'm 25 years old ....  \n",
       "2  \\n      My name is Saad . l am 21 years old . ...  \n",
       "3  \\n      I'm working housewife, cleaner home, c...  \n",
       "4  \\n      Hello,  I'm  Malak , I'm working house...  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the first 5 entries in the corpus:\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5295fd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aux function to add a sentence to the dataframe\n",
    "\n",
    "def add_query_found(corpus, sentence):\n",
    "    sliced = corpus\n",
    "    sliced[\"query_sentence\"] = sentence\n",
    "    return sliced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "16bbeeba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 59466/59466 [01:50<00:00, 537.84it/s]\n"
     ]
    }
   ],
   "source": [
    "df_entry = []\n",
    "\n",
    "for learner in tqdm(range(len(corpus))):\n",
    "    \n",
    "    # Tokenize the passage and convert the tokenize passage into \n",
    "    # tags.\n",
    "    \n",
    "    tokenize_paragraph = nltk.word_tokenize(corpus['passage'][learner])\n",
    "    tagged_paragraph = nltk.pos_tag(tokenize_paragraph)\n",
    "    \n",
    "    # Using the '.' grammar tag to parse passages into sentences.\n",
    "    # This returns the position of the end points in the tagged paragraphs.\n",
    "    \n",
    "    endline_position = [i + 1 for i, v in enumerate(tagged_paragraph) if v[1] == '.']\n",
    "    \n",
    "    # For each identified sentence, we parse out one sentence.\n",
    "    \n",
    "    for index in range(len(endline_position)):\n",
    "        if index == 0:\n",
    "            tagged_sentence = tagged_paragraph[:endline_position[index]]\n",
    "            sentence = tokenize_paragraph[:endline_position[index]]\n",
    "        else:\n",
    "            tagged_sentence = tagged_paragraph[endline_position[index-1]:endline_position[index]]\n",
    "            sentence = tokenize_paragraph[endline_position[index-1]:endline_position[index]]\n",
    "        \n",
    "        \n",
    "        ###==================[PARSE SENTENCE STRUCTURE HERE]====================\n",
    "        # Can be changed to tagged_sentence[0][1] = 'VBZ'\n",
    "        # tagged_sentence[-1][0] != '?' removed sentences ending with question\n",
    "        # \n",
    "        if tagged_sentence[0][1][0:3] =='VBZ' and tagged_sentence[-1][0] != '?':\n",
    "            df_entry.append(add_query_found(corpus.loc[learner], ' '.join(sentence)))\n",
    "            \n",
    "query_df = pd.DataFrame(df_entry)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3932185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df = pd.DataFrame(df_entry)\n",
    "query_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f7825d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', None)\n",
    "# query_df[\"query_sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c5686503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the found sentences out as a csv.\n",
    "# add query_df[[\"learner_id\",\"query_sentence\"]] to output a subset to csv.\n",
    "\n",
    "query_df[[\"query_sentence\"]] .to_csv('{}/output.csv'.format(file_loc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
